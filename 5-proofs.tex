

\subsection{Auxiliary facts about chi-square distributions}

We first establish some auxiliary facts about chi-square distributions, used in the proofs of Theorem \ref{thm:chi-squred-strong-boundary} and Theorem \ref{thm:chi-squred-weak-boundary}.

\begin{lemma}[Rapid variation of chi-square distribution tails] \label{lemma:rapid-variation-chisq}
The central chi-square distribution with $\nu$ degrees of freedom has rapidly varying tails.
That is, 
\begin{equation} \label{eq:rapid-variation-chisq}
    \lim_{x\to\infty}\frac{\P[\chi_\nu^2(0)>tx]}{\P[\chi_\nu^2(0)>x]} = 
    \begin{cases}
    0, & t > 1 \\
    1, & t = 1 \\
    \infty, & 0 < t < 1.
\end{cases}
\end{equation}
\end{lemma}

\begin{proof}[Proof of Lemma \ref{lemma:rapid-variation-chisq}]
When $\nu=1$, the chi-square distribution reduces to a squared Normal, and \eqref{eq:rapid-variation-chisq} follows from the rapid variation of the standard Normal distribution.
For $\nu\ge2$, we recall the following bound on tail probabilities (see, e.g., \citep{inglot2010inequalities}),
$$
\frac{1}{2}\mathcal{E}_\nu(x) \le \P[\chi_\nu^2(0)>x] \le \frac{x}{(x-\nu+2)\sqrt{\pi}} \mathcal{E}_\nu(x), \quad \nu\ge2,\;x>\nu-2,
$$
where $\mathcal{E}_\nu(x) = \exp\left\{-\frac{1}{2}[(x-\nu-(\nu-2)\log(x/\nu) + \log\nu]\right\}$.
Therefore, we have 
$$
\frac{(tx-\nu+2)\sqrt{\pi}}{2tx}\frac{\mathcal{E}_\nu(tx)}{\mathcal{E}_\nu(x)} 
\le \frac{\P[\chi_\nu^2(0)>tx]}{\P[\chi_\nu^2(0)>x]}
\le \frac{2tx}{(tx-\nu+2)\sqrt{\pi}}\frac{\mathcal{E}_\nu(tx)}{\mathcal{E}_\nu(x)},
$$
where ${\mathcal{E}_\nu(tx)}/{\mathcal{E}_\nu(x)} = \exp\{-\frac{1}{2}[(t-1)x-(\nu-2)\log{t}]\}$ converges to $0$ or $\infty$ depending on whether $t>1$ or $0<t<1$.
The case where $t=1$ is trivial.
\end{proof}

\begin{corollary} \label{cor:relative-stability}
Maxima of independent observations from central chi-square distributions with $\nu$ degrees of freedom are relatively stable. 
Specifically, let $\epsilon_p = \left(\epsilon_p(j)\right)_{j=1}^p$ be a i.i.d.\ $\chi_\nu^2(0)$. 
Define the sequence $(u_p)_{p=1}^\infty$ to be the $(1-1/p)$-th generalized quantiles, i.e., 
\begin{equation} \label{eq:quantiles}
    u_p = F^\leftarrow(1 - 1/p),
\end{equation}
where $F$ is the central chi-square distributions with $\nu$ degrees of freedom.
The triangular array ${\cal E} = \{\epsilon_p, p\in\N\}$ has relatively stable (RS) maxima, i.e.,
\begin{equation} \label{eq:RS-condition}
    \frac{1}{u_{p}} M_p := \frac{1}{u_{p}} \max_{j=1,\ldots,p} \epsilon_p(j) \xrightarrow{\P} 1,
\end{equation}
as $p\to\infty$.
\end{corollary}

\begin{proof}[Proof of Corollary \ref{cor:relative-stability}]
When $F(x)<1$ for all finite $x$, \citet{gnedenko1943distribution} showed that the distribution $F$ has rapidly varying tails if and only if the maxima of independent observations from $F$ are relatively stable.
Rapid variation follows from Lemma \ref{lemma:rapid-variation-chisq}.
\end{proof}


\begin{lemma}[Stochastic monotonicity] \label{lemma:stochastic-monotonicity}
The non-central chi-square distribution is stochastically monotone in its non-centrality parameter.
Specifically, for two non-central chi-square distributions both with $\nu$ degrees of freedom, and non-centrality parameters $\lambda_1 \le \lambda_2$, we have $\chi^2_\nu(\lambda_1) \stackrel{\mathrm{d}}{\le} \chi^2_\nu(\lambda_2)$. 
That is,
\begin{equation} \label{eq:stochastic-monotonicity}
    \P[\chi^2_\nu(\lambda_1) \le t] \ge \P[\chi^2_\nu(\lambda_2) \le t], \quad \text{for any}\quad t\ge0.
\end{equation}
\end{lemma}

\begin{proof}[Proof of Lemma \ref{lemma:stochastic-monotonicity}]
Recall that non-central chi-square distributions can be written as sums of $\nu-1$ standard normal random variables and a non-central normal random variable with mean $\sqrt{\lambda}$ and variance 1,
\begin{equation*}
    \chi_\nu^2(\lambda) 
    \stackrel{\mathrm{d}}{=} Z_1^2 + \ldots + Z_{\nu-1}^2 + (Z_\nu + \sqrt{\lambda})^2.
\end{equation*}
Therefore, it suffices to show that $\P[(Z+\sqrt{\lambda})^2 \le t]$ is non-increasing in $\lambda$ for any $t\ge0$, where $Z$ is a standard normal random variable.
We rewrite this expression in terms of standard normal probability function $\Phi$,
\begin{align}
    \P[(Z+\sqrt{\lambda})^2 \le t] 
    &= \P[-\sqrt{\lambda} - \sqrt{t} \le Z \le -\sqrt{\lambda} + \sqrt{t}] \nonumber \\
    &= \Phi(-\sqrt{\lambda} + \sqrt{t}) - \Phi(-\sqrt{\lambda} - \sqrt{t}). \label{eq:stochastic-monotonicity-proof-1}
\end{align}
The derivative of the last expression (with respect to $\lambda$) is 
\begin{equation} \label{eq:stochastic-monotonicity-proof-2}
    \frac{1}{2\sqrt{\lambda}} \left(\phi(\sqrt{\lambda} + \sqrt{t}) - \phi(\sqrt{\lambda} - \sqrt{t})\right) 
    = \frac{1}{2\sqrt{\lambda}} \left(\phi(\sqrt{\lambda} + \sqrt{t}) - \phi(\sqrt{t} - \sqrt{\lambda})\right),
\end{equation}
where $\phi$ is the density of the standard normal distribution.
Notice that we have used the symmetry of $\phi$ around 0 in the last expression.

Since $0 \le \max\{\sqrt{\lambda} - \sqrt{t}, \sqrt{t} - \sqrt{\lambda}\} < \sqrt{t} + \sqrt{\lambda}$ when $t>0$, by monotonicity of the normal density on $(0,\infty)$, we conclude that the derivative \eqref{eq:stochastic-monotonicity-proof-2} is indeed negative.
Therefore, \eqref{eq:stochastic-monotonicity-proof-1} is decreasing in $\lambda$, and \eqref{eq:stochastic-monotonicity} follows for $t>0$.
For $t = 0$, equality holds in \eqref{eq:stochastic-monotonicity} with both probabilities being 0.
\end{proof}


Finally, we derive asymptotic expressions for  chi-square quantiles.

\begin{lemma}[Chi-square quantiles] \label{lemma:chisq-quantiles}
Let $F$ be the central chi-square distributions with $\nu$ degrees of freedom, and let $u(y)$ be the $(1-y)$-th generalized quantile of $F$, i.e.,
\begin{equation} \label{eq:quantiles-generic}
    u(y) = F^\leftarrow(1 - y).
\end{equation}
Then 
\begin{equation}
    u(y) \sim 2\log(1/y), \quad \text{as }y\to0. 
\end{equation}
\end{lemma}

\begin{proof}[Proof of Lemma \ref{lemma:chisq-quantiles}]
The case where $\nu=1$ follows from the well-known Normal quantiles 
$$
F^\leftarrow(1 - 1/p) = \Phi^\leftarrow(1-1/(2p))\sim\sqrt{2\log{(2p)}}\sim\sqrt{2\log{p}}.
$$
The case where $\nu\ge2$ follows from the following estimates of high quantiles of chi-square distributions (see, e.g., \citep{inglot2010inequalities}),
$$
    \nu +  2\log(1/y) -5/2 \le u(y) \le \nu +  2\log(1/y) + 2\sqrt{\nu\log(1/y)}, \quad \text{for all }y\le0.17.
$$
\end{proof}


\subsection{Proof of Theorem \ref{thm:chi-squred-strong-boundary}}
\label{subsec:proof-chi-squred-strong-boundary}

\begin{proof}[Proof of Theorem \ref{thm:chi-squred-strong-boundary}]
We first prove the sufficient condition.
The condition $\underline{r} > {{g}}(\beta)$ implies, after some algebraic manipulation,
$\sqrt{\underline{r}} -\sqrt{1-\beta} > 1$.
Therefore, we can pick $q>1$ such that 
\begin{equation} \label{eq:choice-of-q}
    \sqrt{\underline{r}} -\sqrt{1-\beta} > \sqrt{q} > 1.
\end{equation}
The Bonferroni procedure sets the threshold at $t_p = F^\leftarrow(1-\alpha/p)$, which, by Lemma \ref{lemma:chisq-quantiles}, is asymptotic to $2\log{p} - 2\log{\alpha}$.
By the assumption on $\alpha$ in \eqref{eq:slowly-vanishing-error}, for any $\delta>0$, we have $\alpha\gg p^{-\delta}$ for large $p$.
Therefore, $-\log\alpha\ll\delta\log{p}$ eventually, and we have
$$
1 \le \limsup_{p\to\infty}\frac{2\log{p} - 2\log{\alpha}}{2\log{p}} \le 1+\delta,
$$
for any $\delta>0$.
Hence, $t_p\sim 2\log{p}$.
Setting the $t^* = t^*_p = 2q\log{p}$, we have $t_p < t^*_p$ for large $p$.


On the one hand, $\text{FWER} = 1 - \P[\widehat{S}_p \subseteq S_p]$ vanishes under the Bonferroni procedure with $\alpha\to0$.
On the other hand, for large $p$, the probability of no missed detection is bounded from below by
\begin{equation} \label{eq:chi-square-sufficient-1}
    \P[\widehat{S}_p \supseteq S_p] 
    = \P[\min_{i\in S} x(i) \ge t_p] 
    \ge \P[\min_{i\in S} x(i) \ge t^*] 
    \ge 1 - p^{1-\beta}\P[\chi_\nu^2(\underline{\Delta}) \le t^*],
\end{equation}
where we have used the fact that signal sizes are bounded below by $\underline{\Delta}$, and the stochastic monotonicity of chi-square distributions (Lemma \ref{lemma:stochastic-monotonicity}) in the last inequality.
Writing
$$
\chi_\nu^2(\underline{\Delta}) \stackrel{\mathrm{d}}{=} Z_1^2 + \ldots + Z_{\nu-1}^2 + (Z_\nu + \sqrt{\underline{\Delta}})^2
$$
where $Z_i$'s are iid standard normal variables, we have
\begin{align}
    \P[\chi_\nu^2({\underline{\Delta}}) \le t^*]
    &\le \P[(Z_\nu+\sqrt{\underline{\Delta}})^2 \le t^*] 
    = \P[|Z_\nu+\sqrt{\underline{\Delta}}| \le \sqrt{t^*}]  \nonumber \\
    &\le \P\left[Z_\nu \le - \sqrt{\underline{\Delta}} +  \sqrt{t^*}\right] \nonumber \\
    &= \P\left[Z_\nu \le \sqrt{2\log{p}}\left(\sqrt{q} - \sqrt{\underline{r}}\right)\right]. \label{eq:chi-square-sufficient-2}
\end{align}
By our choice of $q$ in \eqref{eq:choice-of-q}, the last probability in \eqref{eq:chi-square-sufficient-2}
is bounded above by 
$$
\P\Big[Z_\nu \le -\sqrt{2(1-\beta)\log{p}}\Big] = o\left(p^{-(1-\beta)}\right).
$$
This, combined with \eqref{eq:chi-square-sufficient-1}, completes the proof of the sufficient condition for the Bonferroni's procedure.

Under the assumption of independence, Sid\'ak's, Holm's, and Hochberg's procedures are strictly more powerful than Bonferroni's procedure, while controlling FWER at the nominal levels.
Therefore, the risks of exact support recovery for these procedures also vanishes.
This completes the proof for the first part of Theorem \ref{thm:chi-squred-strong-boundary}.

We now show the necessary condition. 
We first normalize the maxima by the chi-square quantiles $u_p = F^{\leftarrow}(1-1/p)$, where $F$ is the distribution of a (central) chi-square random variable,
\begin{equation} \label{eq:chi-square-necessary-0}
 \P[\widehat{S}_p = S_p] \le \P\left[\max_{i\in S^c}x(i) < \min_{i\in S}x(i)\right]
  % &= \P\left[\frac{\max_{i\in S^c}x(i)}{u_p} < \frac{\min_{i\in S}x(i)}{u_p}\right] \nonumber \\
  % &\le  \P\left[\frac{\max_{i\in S^c}\chi_\nu^2(\lambda(i))}{u_p} < \frac{\min_{i\in S}\chi_\nu^2(\lambda(i))}{u_p}\right] \nonumber \\
  = \P\left[ \frac{M_{S^c}}{u_p} < \frac{m_S}{u_p} \right],
\end{equation}
where $M_{S^c} = \max_{i\in S^c}\chi_\nu^2(\lambda(i))$ and $m_{S} = \min_{i\in S}\chi_\nu^2(\lambda(i))$.
Writing
\begin{equation} \label{eq:chi-square-necessary-1}
    \frac{M_{S^c}}{u_{p}} = \frac{M_{S^c}}{u_{p-p^{1-\beta}}} \frac{u_{p-p^{1-\beta}}}{u_{p}}, % \stackrel{\P}{\longrightarrow} 1,
\end{equation}
we know by the relative stability of chi-square random variables (Corollary \ref{cor:relative-stability}) that the first term in the product converges to 1 in probability.
The second term, using the expression for chi-square quantiles (Lemma \ref{lemma:chisq-quantiles}), is asymptotic to 
$$
\frac{2\log{(p-p^{1-\beta})}}{2\log{p}} = \frac{\log{p}+\log{(1-p^{-\beta})}}{\log{p}} \sim 1.
$$
Therefore, the expression \eqref{eq:chi-square-necessary-1} converges to $1$ in probability.

Meanwhile, for any $i\in S$, by Lemma \ref{lemma:stochastic-monotonicity} and the fact that signal sizes are bounded above by $\overline{\Delta}$, we have,
\begin{equation*}
    {\chi_\nu^2(\lambda(i))} \stackrel{\mathrm{d}}{\le}
    {\chi_\nu^2(\overline{\Delta})} \stackrel{\mathrm{d}}{=} 
    {Z_1^2 + \ldots + Z_{\nu-1}^2 + \left(Z_\nu + \sqrt{\overline{\Delta}}\right)^2}.
\end{equation*}
Dividing through by $u_p$, and taking minimum over $S$, we obtain
\begin{equation} \label{eq:chi-square-necessary-3}
    \frac{m_S}{u_p} 
    = \min_{i\in S} \frac{\chi_\nu^2(\lambda(i))}{u_p} 
    % \stackrel{\mathrm{d}}{\le} \min \left\{\frac{\chi_\nu^2(\overline{\Delta})}{u_p}, s \text{ i.i.d. copies} \right\} \\
    \stackrel{\mathrm{d}}{\le} \min_{i\in S}\frac{Z_1^2(i) + \ldots + Z_{\nu-1}^2(i)}{u_p} + \frac{(Z_\nu(i) + \sqrt{\overline{\Delta}})^2}{u_p}.
\end{equation}
Let $i^\dagger = i^\dagger_p$ be the index minimizing the second term in \eqref{eq:chi-square-necessary-3}, i.e.,
\begin{equation}
    i^\dagger := \argmin_{i\in S} \frac{(Z_\nu(i) + \sqrt{\overline{\Delta}})^2}{u_p}
    = \argmin_{i\in S} \frac{(Z_\nu(i) + \sqrt{\overline{\Delta}})^2}{2\log{p}},
\end{equation}
and define the function $f_p(x):=(x+\sqrt{\overline{\Delta}})^2/(2\log{p})$, we shall first show that 
\begin{equation} \label{eq:chi-square-necessary-4}
    %\min_{i\in S} \frac{(Z_\nu(i) + \sqrt{\overline{\Delta}})^2}{2\log{p}} 
    \P[ f_p(Z_\nu(i^\dagger)) < 1 ] \to 1.
\end{equation}
On the one hand, we know (by solving a quadratic inequality) that
\begin{equation} \label{eq:chi-square-necessary-5}
    f_p(x)<1 \iff \frac{x}{\sqrt{2\log{p}}} \in (-(\sqrt{\overline{r}}+1), -(\sqrt{\overline{r}}-1)).
\end{equation}
On the other hand, we know (by relative stability of i.i.d. Gaussians) that 
\begin{equation} \label{eq:chi-square-necessary-6}
    \frac{\min_{i\in S} Z_\nu(i)}{\sqrt{2\log{p}}}
    \to -\sqrt{1-\beta} \quad\text{in probability}.
\end{equation}
Further, by the assumption on the signal sizes $\overline{r} < (1+\sqrt{1-\beta})^2$, we have,
\begin{equation} \label{eq:chi-square-necessary-7}
    -(\sqrt{\overline{r}}+1) < - \sqrt{1-\beta} < - (\sqrt{\overline{r}}-1).
\end{equation}
Combining \eqref{eq:chi-square-necessary-5}, \eqref{eq:chi-square-necessary-6}, and \eqref{eq:chi-square-necessary-7}, we obtain
$$
\P\left[\min_{i\in S} f_p(Z_\nu(i)) < 1\right]
= \P\left[ f_p(Z_\nu(i^\dagger)) < 1 \right] 
\ge \P\left[ f_p\left(\min_{i\in S}Z_nu(i)\right) < 1 \right] \to 1,
$$
and we arrive at \eqref{eq:chi-square-necessary-4}.
As a corollary, since $u_p\sim2\log{p}$, it follows that
\begin{equation} \label{eq:chi-square-necessary-8}
    \P\left[\min_{i\in S}\frac{(Z_\nu(i) + \sqrt{\overline{\Delta}})^2}{u_p} < 1\right]\to1.
\end{equation}

Finally, by independence between $Z_1^2(i)+\ldots+Z_{\nu-1}^2(i)$ and $(z_\nu^2(i)+\sqrt{\overline{\Delta}})^2$, and the fact that $i^\dagger$ is a function of only the second term, we have
$$
Z_1^2(i^\dagger)+\ldots+Z_{\nu-1}^2(i^\dagger) 
\stackrel{\mathrm{d}}{=} Z_1^2(i)+\ldots+Z_{\nu-1}^2(i) 
\quad \text{for all} \;\; i\in S.
$$
Therefore, $Z_1^2(i^\dagger)+\ldots+Z_{\nu-1}^2(i^\dagger) = O_\P(1)$, and 
\begin{equation} \label{eq:chi-square-necessary-9}
    \frac{Z_1^2(i^\dagger)+\ldots+Z_{\nu-1}^2(i^\dagger)}{u_p} \to 0 \quad \text{in probability}. 
\end{equation}
Together, \eqref{eq:chi-square-necessary-8} and \eqref{eq:chi-square-necessary-9} imply that
\begin{align}
    \P\left[\frac{m_S}{u_p}<1\right]
    &\ge \P\left[\min_{i\in S}\frac{Z_1^2(i) + \ldots + Z_{\nu-1}^2(i)}{u_p} + \frac{(Z_\nu(i) + \sqrt{\overline{\Delta}})^2}{u_p} < 1\right] \nonumber \\
    &\ge \P\left[\frac{Z_1^2(i^\dagger) + \ldots + Z_{\nu-1}^2(i^\dagger)}{u_p} + \frac{(Z_\nu(i^\dagger) + \sqrt{\overline{\Delta}})^2}{u_p} < 1\right] \to 1. \label{eq:chi-square-necessary-10}
\end{align}
In view of \eqref{eq:chi-square-necessary-0}, \eqref{eq:chi-square-necessary-1}, and \eqref{eq:chi-square-necessary-10}, we conclude that exact recovery cannot succeed with any positive probability.
The proof of the necessary condition is complete.
\end{proof}

\subsection{Proof of Theorem \ref{thm:chi-squred-weak-boundary}}
\label{subsec:proof-chi-squred-weak-boundary}

We first show the necessary condition. 
That is, when $\overline{r} < \beta$, no thresholding procedure is able to achieve approximate support recovery.
The main structure of the proof follows that of Theorem 1 in \citet{arias2017distribution}. 
Our arguments, however, allow for unequal signal sizes; these arguments can in turn be used to generalize the results in \cite{arias2017distribution}.

\begin{proof}[Proof of necessary condition in Theorem \ref{thm:chi-squred-weak-boundary}]
Denote the distributions of $\chi^2_\nu(0)$, $\chi^2_\nu(\underline{\Delta})$ and $\chi^2_\nu(\overline{\Delta})$ as $F_0$, $F_{\underline{a}}$, and $F_{\overline{a}}$ respectively.

We first show the necessary condition, i.e., when $\overline{r}<\beta$, approximate support recovery cannot be achieved with any thresholding procedure.
In particular, we show that the liminf of the sum of FDP and NDP is at least 1.

Recall that thresholding procedures are of the form
$$
\widehat{S}_p = \left\{i\,|\,x(i) > t_p(x)\right\}.
$$
Denote $\widehat{S} := \left\{i\,|\,x(i) > t_p(x)\right\}$, and $\widehat{S}(u) := \left\{i\,|\,x(i) > u\right\}$.
For any threshold $u\ge t_p$ we must have $\widehat{S}(u)\subseteq\widehat{S}$, and hence
\begin{equation} \label{eq:weak-boundary-proof-FDP}
    \text{FDP} = \frac{|\widehat{S}\setminus{S}|}{|\widehat{S}|} \ge \frac{|\widehat{S}\setminus{S}|}{|\widehat{S}\cup{S}|} = \frac{|\widehat{S}\setminus{S}|}{|\widehat{S}\setminus{S}| + |S|} \ge
    \frac{|\widehat{S}(u)\setminus{S}|}{|\widehat{S}(u)\setminus{S}| + |S|}.
\end{equation}
On the other hand, for any threshold $u\le t_p$ we must have $\widehat{S}(u)\supseteq\widehat{S}$, and hence
\begin{equation} \label{eq:weak-boundary-proof-NDP}
    \text{NDP} = \frac{|{S}\setminus\widehat{S}|}{|{S}|} \ge 
    \frac{|{S}\setminus\widehat{S}(u)|}{|{S}|}.
\end{equation}
Since either $u\ge t_p$ or  $u\le t_p$ must take place, putting \eqref{eq:weak-boundary-proof-FDP} and \eqref{eq:weak-boundary-proof-NDP} together, we have
\begin{equation} \label{eq:weak-boundary-proof-converse-1}
    \text{FDP} + \text{NDP} 
    \ge \frac{|\widehat{S}(u)\setminus{S}|}{|\widehat{S}(u)\setminus{S}|+|{S}|} \wedge \frac{|{S}\setminus\widehat{S}(u)|}{|{S}|},
\end{equation}
for any $u$.
Therefore it suffices to show that for a suitable choice of $u$, the RHS of \eqref{eq:weak-boundary-proof-converse-1} converges to 1.

Let $t^* = 2q\log{p}$, where $\overline{r}<q<\beta$, we obtain an estimate of the tail probability
\begin{align}
    \overline{F_0}(t^*) 
    &= \P[\chi_\nu^2(0) > t^*] 
    = \frac{2^{1-\nu/2}}{\Gamma(\nu/2)} \int_{2q\log{p}}^\infty x^{\nu/2-1}e^{-x/2} \mathrm{d}x \nonumber \\
    &\sim \frac{2^{1-\nu/2}}{\Gamma(\nu/2)} \left(2q\log{p}\right)^{\nu/2-1}p^{-q}, \label{eq:weak-boundary-proof-null-tail-prob}
\end{align}
where $a_p\sim b_p$ is taken to mean $a_p/b_p\to 1$.
Observe that $|\widehat{S}(t^*)\setminus{S}|$ has distribution $\text{Binom}(p-s, \overline{F_0}(t^*))$, denote $X = X_p := {|\widehat{S}(t^*)\setminus{S}|}/{|S|}$, and we have 
$$
\mu := \E\left[X\right] = \frac{(p-s)\overline{F_0}(t^*)}{s},
\quad \text{and} \quad
\var\left(X\right) = \frac{(p-s)\overline{F_0}(t^*){F_0}(t^*)}{s^2} \le \mu/s.
$$
Therefore for any $M>0$, we have, by Chebyshev's inequality,
\begin{equation}
    \P\left[X < M\right] 
    \le \P\left[\left|X-\mu\right| > \mu - M\right]
    \le \frac{\mu/s}{(\mu-M)^2}
    = \frac{1/(\mu s)}{(1-M/\mu)^2}. \label{eq:weak-boundary-proof-converse-2}
\end{equation}
Now, from the expression of $\overline{F_0}(t^*)$ in \eqref{eq:weak-boundary-proof-null-tail-prob}, we obtain
$$
\mu = (p^\beta - 1)\overline{F_0}(t^*) \sim \frac{2^{1-\nu/2}}{\Gamma(\nu/2)} \left(2q\log{p}\right)^{\nu/2-1}p^{\beta-q}\to\infty \quad \text{ as }\;p\to\infty.
$$
Therefore the last expression in \eqref{eq:weak-boundary-proof-converse-2} converges to 0, and we conclude that $X\to\infty$ in probability, and hence
$$
\frac{|\widehat{S}(t^*)\setminus{S}|}{|\widehat{S}(t^*)\setminus{S}|+|{S}|} 
= \frac{X}{X+1} \to 1 \quad \text{in probability}.
$$

On the other hand, we show that with the same choice of $u = t^*$,
$$
\frac{|{S}\setminus\widehat{S}(t^*)|}{|{S}|}\to 1 \quad \text{in probability}.
$$
By the stochastic monotonicity of chi-square distributions (Lemma \ref{lemma:stochastic-monotonicity}), the probability of missed detection for each signal is lower bounded by $\P[\chi^2_\nu(\lambda_i) \le t^*] \ge F_{\overline{a}}(t^*))$.
Therefore, $|{S}\setminus\widehat{S}(t^*)| \stackrel{\mathrm{d}}{\ge} \text{Binom}(s, {F_{\overline{a}}}(t^*))$, and it suffices to show that ${F_{\overline{a}}}(t^*)$ converges to 1.
This is indeed the case, since
\begin{align*}
    {F_{\overline{a}}}(t^*) 
    &= \P[Z_1^2 + \ldots + Z_\nu^2 + 2\sqrt{2\overline{r}\log{p}} Z_\nu + 2\overline{r}\log{p} \le 2q\log{p}] \\
    &\ge \P[Z_1^2 + \ldots + Z_\nu^2 \le (q-\overline{r})\log{p}, \; 2\sqrt{2\overline{r}\log{p}} Z_\nu \le (q-\overline{r})\log{p}],
\end{align*}
and both events in the last line have probability going to 1 as $p\to\infty$.
The necessary condition is shown.
\end{proof}

We now turn to the sufficient condition. 
That is, when $\underline{r} > \beta$, the Benjamini-Hochberg procedure with slowly vanishing FDR levels achieves asymptotic approximate support recovery.

The proof proceeds in two steps.
We first make the connection between power of the BH procedure and stochastic ordering of the alternatives.
It is formalized in the following lemma, which could be of independent interest.
This result, though natural, seems new. 

\begin{lemma}[Monotonicity of the BH procedure] \label{lemma:monotonicity-BH-procedure}
Compare Alternatives 1 and 2 where we have $p$ independent observations $x(i)$, $i\in\{1,\ldots,p\}$,
\begin{enumerate}
    \item[Alt.1] the $p-s$ coordinates in the null part have common distribution $F_0$, and the $s$ signals have alternative distributions $F^{i}_1$, $i\in S$, respectively.
    \item[Alt.2] the $p-s$ coordinates in the null part have common distribution $F_0$, and the $s$ signals have alternative distributions $F^{i}_2$, $i\in S$, respectively, where
    $$ F^{i}_2(t) \le F^{i}_1(t), \quad \text{for all} \;\; t\in\R, \; \text{and for all} \;\; i\in S.$$
\end{enumerate}
If we apply the BH procedure at the same nominal level of FDR $\alpha$, then the FNR under Alternative 2 is bounded above by the FNR under Alternative 1.
\end{lemma}

Loosely put, the power of the BH procedure is monotone increasing with respect to the stochastic ordering of the alternatives.

\begin{proof}[Proof of Lemma \ref{lemma:monotonicity-BH-procedure}]
We first re-express the BH procedure in a different form.
Recall that on observing $x(i)$, $i\in\{1,\ldots,p\}$, the BH procedure is the thresholding procedure with threshold set at $x_{[i^*]}$, where $i^* := \max\{i\,|\,\overline{F_0}(x_{[i]})\le \alpha i/p\}$, and $x_{[1]}\ge\ldots\ge x_{[p]}$ are the order statistics.

Let $\widehat{G}$ denote the empirical survival function
\begin{equation} \label{eq:empirical-tail-distribution}
    \widehat{G}(t) = \frac{1}{p}\sum_{i\in[p]}\mathbbm{1}\{x(i) \ge t\}.
\end{equation}
By the definition, we know that $\widehat{G}(x_{[i]}) = i/p$.
Therefore, by the definition of $i^*$, we have
\begin{equation*} 
    \overline{F_0}(x_{[i]}) > \alpha\widehat{G}(x_{[i]}) = \alpha i/p \quad \text{for all }i>i^*.
\end{equation*}
Since $\widehat{G}$ is constant on $(x_{[i^*+1]}, x_{[i^*]}]$, the fact that 
$\overline{F_0}(x_{[i^*]}) \le \alpha\widehat{G}(x_{[i^*]})$ and $\overline{F_0}(x_{[i^*+1]}) > \alpha\widehat{G}(x_{[i^*+1]})$ implies that $\alpha\widehat{G}$ and $\overline{F_0}$ must ``intersect'' on the interval.
We denote this ``intersection'' as
\begin{equation} \label{eq:weak-boundary-proof-tau}
    \tau = \inf\{t\,|\,\overline{F_0}(t)\le\alpha\widehat{G}(t)\}. 
    %= \min\{t\,|\,\overline{F_0}(t)=\alpha\widehat{G}(t)\}.
\end{equation}
Note that $\tau$ cannot be equal to $x_{[i^*+1]}$ since $\overline{F}_0$ is C\`adl\`ag.
Since there is no observation in $[\tau, x_{[i^*]})$, we can write the BH procedure as the thresholding procedure with threshold set at $\tau$.

Now, denote the observations under Alternatives 1 and 2 as $x_1(i)$ and $x_2(i)$.
Since $x_2(i)$ stochastically dominates $x_1(i)$ for all $i\in\{1,\ldots,p\}$, there exists a coupling $(\widetilde{x}_1, \widetilde{x}_2)$ of $x_1$ and $x_2$ such that 
% $\widetilde{x}_1(i) = \widetilde{x}_2(i)$ for $i\in S^c$, and 
$\widetilde{x}_1(i) \le \widetilde{x}_2(i)$ a.s. for all $i$.
We will replace $x_1$ and $x_2$ with $\widetilde{x}_1$ and $\widetilde{x}_2$ in what follows.
Since we will compare expectations with respect to $\widetilde{x}$'s in the last step, this replacement does not cause any trouble.
To simplify notation, we still write $x_1$ and $x_2$ in place of $\widetilde{x}_1$ and $\widetilde{x}_2$.

Let $\widehat{G}_k$ be the empirical survival function under Alternative $k$, i.e.,
\begin{equation} \label{eq:empirical-survival}
    \widehat{G}_k(t) = \frac{1}{p}\sum_{i\in[p]}\mathbbm{1}\{x_k(i) \ge t\}, \quad k\in\{1,2\}.
\end{equation}
We define the BH thresholds $\tau_1$ and $\tau_2$ by replacing $\widehat{G}$ in \eqref{eq:weak-boundary-proof-tau} with $\widehat{G}_1$ and $\widehat{G}_2$, respectively.
Denote the set estimates of signal support $\widehat{S}_k = \{i\,|\,x_k(i)\ge\tau_k\}$ by the BH procedure.
We claim that (perhaps surprisingly),
\begin{equation} \label{eq:monotonicity-BH-procedure-thresholds}
    \tau_2 \le \tau_1 \quad \text{with probability } 1.
\end{equation}

Indeed, by definition of the empirical survival function \eqref{eq:empirical-survival} and the fact that $x_1(i) \le x_2(i)$ almost surely for all $i$,  we have $\widehat{G}_1(t) \le \widehat{G}_2(t)$ for all $t$.
Hence, $\overline{F_0}(t)\le\alpha\widehat{G}_1(t)$ implies $\overline{F_0}(t)\le\alpha\widehat{G}_2(t)$, and Relation \eqref{eq:monotonicity-BH-procedure-thresholds} follows from the definition of $\tau$ in \eqref{eq:weak-boundary-proof-tau}.

Finally, when $\tau_2 \le \tau_1$, we have $\tau_2 \le \tau_1 \le x_1(i) \le x_2(i)$ with probability 1 for all $i\in\widehat{S}_1$.
Therefore, it follows that $\widehat{S}_1 \subseteq \widehat{S}_2$ and hence $|S\setminus\widehat{S}_2| \le |S\setminus\widehat{S}_1|$ almost surely. 
The conclusion in Lemma \ref{lemma:monotonicity-BH-procedure} follows from the last inequality.
\end{proof}

Lemma \ref{lemma:monotonicity-BH-procedure} allows us to reduce the analysis of alternative with unequal signal sizes to one with equal signal sizes. 
The structure for the rest of the proof for the sufficient condition follows that of Theorem 2 in \cite{arias2017distribution}. 

\begin{proof}[Proof of sufficient condition in Theorem \ref{thm:chi-squred-weak-boundary}]
The FDR vanishes by our choice of $\alpha$ and the FDR-controlling property of the BH procedure.
It only remains to show that FNR also vanishes.

To do so we compare the FNR under the alternative specified in Theorem \ref{thm:chi-squred-weak-boundary} to one with all of the signal sizes equal to $\underline{\Delta}$.
Let $x(i)$ be vectors of independent observations with $p-s$ nulls having $\chi^2_\nu(0)$ distributions, and $s$ signals having $\chi^2_\nu(\underline{\Delta})$ distributions.
By Lemma \ref{lemma:stochastic-monotonicity} Lemma \ref{lemma:monotonicity-BH-procedure}, it suffices to show that the FNR under the BH procedure in this setting vanishes.

Let $\widehat{G}$ denote the empirical survival function as in \eqref{eq:empirical-tail-distribution}.
Define the empirical survival functions for the null part and signal part
\begin{equation} \label{eq:empirical-survival-null-signal}
    \widehat{W}_\text{null}(t) = \frac{1}{p-s}\sum_{i\not\in S}\mathbbm{1}\{x(i) \ge t\},
    \quad
    \widehat{W}_\text{signal}(t) = \frac{1}{s}\sum_{i\in S}\mathbbm{1}\{x(i) \ge t\},
\end{equation}
so that
$$
\widehat{G}(t) = \frac{p-s}{p}\widehat{W}_\text{null}(t) + \frac{s}{p}\widehat{W}_\text{signal}(t).
$$

We need the following result to describe the deviations of the empirical distributions.
\begin{lemma}[\cite{eicker1979asymptotic}] \label{lemma:empirical-process}
Let $Z_1,\ldots,Z_k$ be i.i.d.\ with continuous survival function $Q$.
Let $\widehat{Q}_k$ denote their empirical survival function and define 
$\xi_k = \sqrt{2\log{\log{(k)}}/k}$ for $k \ge 3$. 
Then
$$
\frac{1}{\xi_k}\sup_z\frac{\widehat{Q}_k(z) - Q(z)}{\sqrt{Q(z)(1 - Q(z))}} \to 1,
$$
in probability as $k \to \infty$.
In particular,
$$
\widehat{Q}_k(z) = Q(z) + O_\P\left(\xi_k\sqrt{Q(z)(1 - Q(z))}\right),
$$
uniformly in z.
\end{lemma}

Apply Lemma \ref{lemma:empirical-process} to $\widehat{G}$, we obtain
$\widehat{G}(t) = G(t) + \widehat{R}(t)$.
where 
\begin{equation} \label{eq:empirical-process-mean}
    G(t) = \frac{p-s}{p}\overline{F_0}(t) + \frac{s}{p}\overline{F_a}(t),
\end{equation}
where $\overline{F_0}$ and $\overline{F_{a}}$ are the survival functions of $\chi_\nu^2(0)$ and $\chi_\nu^2(\underline{\Delta})$ respectively, and 
\begin{equation} \label{eq:empirical-process-residual}
    \widehat{R}(t) = O_\P\left(\xi_p\sqrt{\overline{F_0}(t)F_0(t)} + \frac{s}{p}\xi_s\sqrt{\overline{F_a}(t)F_a(t)}\right),
\end{equation}
uniformly in $t$.

Recall (see proof of Lemma \ref{lemma:monotonicity-BH-procedure}) that the BH procedure is the thresholding procedure with threshold set at $\tau$ (defined in \eqref{eq:weak-boundary-proof-tau}).
% \begin{equation} \label{eq:weak-boundary-proof-tau-recall}
%     \tau = \inf\{t\,|\,\overline{F_0}(t)\le\alpha\widehat{G}(t)\}. 
%     %= \min\{t\,|\,\overline{F_0}(t)=\alpha\widehat{G}(t)\}.
% \end{equation}
The NDP may also be re-written as 
$$
\text{NDP} = \frac{|{S}\setminus\widehat{S}|}{|{S}|} = \frac{1}{s}\sum_{i\in S}\mathbbm{1}\{x(i) < \tau\} = 1 - \widehat{W}_\text{signal}(\tau),
$$
so that it suffices to show that 
\begin{equation} \label{eq:weak-boundary-proof-sufficient-1}
    \widehat{W}_\text{signal}(\tau)\to 1
\end{equation} in probability.
Applying Lemma \ref{lemma:empirical-process} to $\widehat{W}_\text{signal}$, we know that 
$$
\widehat{W}_\text{signal}(\tau) = \overline{F_a}(\tau) + O_\P\left(\xi_s\sqrt{\overline{F_a}(\tau)F_a(\tau)}\right) = \overline{F_a}(\tau) + o_\P(1).
$$
So it suffices to show that $F_a(\tau)\to 0$ in probability.
Now let $t^* = 2q\log(p)$ for some $q$ such that $\beta<q<r$.
We have 
\begin{equation} \label{eq:weak-boundary-proof-sufficient-2}
    F_a(t^*) = \P[\chi^2_\nu(\underline{\Delta}) \le t^*]
    \le \P\left[2\sqrt{\underline{\Delta}}Z_\nu \le t^* - \underline{\Delta}\right] 
    = \P\left[Z_\nu \le \frac{t^*}{2\sqrt{\underline{\Delta}}} - \frac{\sqrt{\underline{\Delta}}}{2}\right] \to 0.
\end{equation}
Hence in order to show \eqref{eq:weak-boundary-proof-sufficient-1}, it suffices to show 
\begin{equation} \label{eq:weak-boundary-proof-sufficient-3}
    \P\left[\tau \le t^*\right] \to 1.
\end{equation}
By \eqref{eq:empirical-process-mean}, the mean of the empirical process $\widehat{G}$ evaluated at $t^*$ is
\begin{equation*}
    G(t^*) = \frac{p-s}{p}\overline{F_0}(t^*) + \frac{s}{p}\overline{F_a}(t^*).
\end{equation*}
The first term, using Relation \eqref{eq:weak-boundary-proof-null-tail-prob}, is asymptotic to $p^{-q}L(p)$, where $L(p)$ is the logarithmic term in $p$.
The second term, since $\overline{F_a}(t^*)\to 1$ by Relation \eqref{eq:weak-boundary-proof-sufficient-2}, is asymptotic to $p^{-\beta}$.
Therefore, $G(t^*) \sim p^{-q}L(p) + p^{-\beta} \sim p^{-\beta}$, since $q>\beta$ and $p^\delta L(p)$ for all $\delta>0$.

The fluctuation of the empirical process at $t^*$, by Relation \eqref{eq:empirical-process-residual}, is 
\begin{align*}
    \widehat{R}(t^*) 
    &= O_\P\left(\xi_p\sqrt{\overline{F_0}(t^*)F_0(t^*)} + \frac{s}{p}\xi_s\sqrt{\overline{F_a}(t^*)F_a(t^*)}\right)\\
    &= O_\P\left(\xi_p\sqrt{\overline{F_0}(t^*)}\right) + o_\P\left(p^{-\beta}\right).
\end{align*}
By \eqref{eq:weak-boundary-proof-null-tail-prob} and the expression for $\xi_p$, the first term is $O_\P\left(p^{-(q+1)/2}L(p)\right)$ where $L(p)$ is a poly-logarithmic term in $p$.
Since $\beta<\min\{q,1\}$, we have $\beta<(q+1)/2$, and hence $\widehat{R}(t^*) = o_\P(p^{-\beta})$.

Putting the mean and the fluctuation of $\widehat{G}(t^*)$ together, we obtain
$$
\widehat{G}(t^*) = G(t^*) + \widehat{R}(t^*) \sim_\P G(t^*) \sim p^{-\beta},
$$
and therefore, together with \eqref{eq:weak-boundary-proof-null-tail-prob}, we have
$$
\overline{F_0}(t^*)/\widehat{G}(t^*) = p^{\beta-q}L(p)(1+o_{\P}(1)),
$$
which is eventually smaller than the FDR level $\alpha$ by the assumption \eqref{eq:slowly-vanishing-error} and the fact that $\beta<q$.
That is, 
$$
\P\left[\overline{F}_0(t^*) / \widehat{G}(t^*) < \alpha\right] \to 1.
$$
By definition of $\tau$ (recall \eqref{eq:weak-boundary-proof-tau}), this implies that $\tau \le t^*$ with probability tending to 1, and \eqref{eq:weak-boundary-proof-sufficient-3} is shown.
The proof for the sufficient condition is complete.
\end{proof}

\subsection{Signal sizes and odds ratio in 2-by-2 contingency tables}
\label{subsec:proof-signal-size-odds-ratio}

\begin{proof}[Proof of Proposition \ref{prop:signal-size-odds-ratio} and Corollary \ref{cor:signal-limits-OR}]
We parametrize the 2-by-2 multinomial distribution with the parameter $\delta$, 
\begin{equation} \label{eq:reparametrize-2-by-2-table-1}
    \mu_{11} = \phi_1\theta_1+\delta,\quad 
    \mu_{12} = \phi_1\theta_2-\delta,\quad 
    \mu_{21} = \phi_2\theta_1-\delta,\quad 
    \mu_{22} = \phi_2\theta_2+\delta.
\end{equation}
By relabeling of categories, we may assume $0<\theta_1,\phi_1\le1/2$ without loss of generality.
Note that $\delta$ must lie within the range $[\delta_\mathrm{min}, \delta_\mathrm{max}]$, where
$$
\delta_\mathrm{min} := \max\{-\phi_1\theta_1, -\phi_2\theta_2, \phi_1\theta_2-1, \phi_2\theta_1-1\} 
= -\phi_1\theta_1,
$$
and
$$
\delta_\mathrm{max} := \min\{1-\phi_1\theta_1, 1-\phi_2\theta_2, \phi_1\theta_2, \phi_2\theta_1\}
= \min\{\phi_1\theta_2, \phi_2\theta_1\},
$$
in order for $\mu_{ij}\ge0$ for all $i,j\in \{1,2\}$.
Under this parametrization, Relation \eqref{eq:odds-ratio} then becomes
\begin{equation} \label{eq:odds-ratio-delta}
    \text{R} = \frac{\mu_{11}\mu_{22}}{\mu_{12}\mu_{21}}
    = \frac{\phi_1\theta_1\phi_2\theta_2 + \delta(\phi_1\theta_1+\phi_2\theta_2)+\delta^2}{\phi_1\theta_1\phi_2\theta_2 - \delta(\phi_1\theta_2+\phi_2\theta_1)+\delta^2},
\end{equation}
which is one-to-one and increasing in $\delta$ on $(\delta_\mathrm{min}, \delta_\mathrm{max})$.
Equations \eqref{eq:signal-size-chisq} becomes
\begin{equation} \label{eq:signal-size-chisq-delta}
w^2 = \sum_{i=1}^2 \sum_{j=1}^2 \frac{(\mu_{ij} - \phi_i\theta_j)^2}{\phi_i\theta_j}
= \delta^2\sum_i\sum_j \frac{1}{\phi_i\theta_j}
= \frac{\delta^2}{\phi_1\theta_1\phi_2\theta_2},
\end{equation}
Solving for $\delta$ in \eqref{eq:odds-ratio-delta} using $R>0$, and plug into the expression for signal size \eqref{eq:signal-size-chisq-delta} yields Relation \eqref{eq:signal-size-odds-ratio}.
The last claim follows from the fact that $w^2(\delta)$ is decreasing on $[\delta_\mathrm{min},0)$, increasing on $(0,\delta_\mathrm{max}]$, with limits
$$
\lim_{d\to \delta_\mathrm{min}} w^2(\delta) = \frac{\phi_1\theta_1}{\phi_2\theta_2},
\quad
\text{and}
\quad
\lim_{d\to \delta_\mathrm{max}} w^2(\delta) = \min\left\{\frac{\phi_1\theta_2}{\phi_2\theta_1}, \frac{\phi_2\theta_1}{\phi_1\theta_2}\right\}.
$$
The other three cases ($1/2\le\theta_1,\phi_1\le1$, $0<\theta_1\le1/2\le\phi_1\le1$, and $0\le\phi_1\le1/2\le\theta_1\le1$) may be handled similarly, or by observing the symmetry of the problem.
\end{proof}

\begin{proof}[Proof of Corollary \ref{cor:optimal-design}]
Using the parametrization in \eqref{eq:reparametrize-2-by-2-table-1} and Corollay \ref{cor:signal-size-odds-ratio-conditional-frequency}, we have
\begin{align}
    \delta &= \frac{\phi_1 fR}{fR+1-f} - \left(\frac{\phi_1 fR}{fR+1-f} + f(1-\phi_1)\right)\phi_1 \nonumber \\
    &= \frac{f(1-f)\phi_1(1-\phi_1)(R-1)}{fR+1-f}. \label{eq:eq:reparametrize-2-by-2-table-2}
\end{align}
Substituting \eqref{eq:eq:reparametrize-2-by-2-table-2} into the expression \eqref{eq:signal-size-chisq-delta}, after some simplification, yields
\begin{equation} \label{eq:eq:reparametrize-2-by-2-table-3}
    w^2 = \frac{f(1-f)\phi_1(1-\phi_1)(R-1)^2}{\left[\phi_1 R + (1-\phi_1)D\right]\left[\phi_1 + (1-\phi_1)D\right]},
\end{equation}
where $D = fR+1-f$.
The derivative of \eqref{eq:eq:reparametrize-2-by-2-table-3} with respect to $\phi_1$ is
\begin{equation*}
    \frac{\mathrm{d}w^2}{\mathrm{d}\phi_1} = 
    \frac{f(1-f)(R-1)^2}{\left[\phi_1 R+(1-\phi_1)D\right]^2 \left[\phi_1+(1-\phi_1)D\right]^2} \left[(D^2-R)\phi_1^2 - 2D^2\phi_1 + D^2\right],
\end{equation*}
which is strictly positive at $\phi_1=0$ (if $f\neq1$ and hence $D\neq0$), and strictly negative at $\phi_1=1$.
Further, the second derivative
\begin{equation*}
    \frac{\mathrm{d}^2w^2}{\mathrm{d}\phi_1^2} = 
    h(R,f) \left[(D^2-R)\phi_1 - D^2\right],
\end{equation*}
where $h$ is a function of $(R,f)$ taking positive values, is strictly negative on $[0,1]$.
This implies that the solution to $(D^2-R)\phi_1^2 - 2D^2\phi_1 + D^2 = 0$ in the interval of $[0,1]$ is the maximizer of \eqref{eq:eq:reparametrize-2-by-2-table-3}.
When $D^2-R>0$, this is the smaller of the two roots, and when $D^2-R<0$, the larger of the two;
they share the same expression ${D}/{(D+\sqrt{R})}$, which coincides with \eqref{eq:optimal-design}.
Finally, when $D^2=R$, the only root $\phi_1^*=1/2$ is the maximizer of \eqref{eq:eq:reparametrize-2-by-2-table-3}.
\end{proof}
