
We shall work under the asymptotic regime where the dimensionality $p$ tends to infinity.
Specifically, we will work with the triangular array of chi-square models \eqref{eq:model-chisq} indexed by $p$.
Let the non-centrality parameter vectors $\lambda = \lambda_p$ have 
\begin{equation} \label{eq:signal-sparsity}
    |S_p| = \left\lfloor p^{1-\beta} \right\rfloor
\end{equation}
non-zero entries, where the non-zero parameters $\lambda(i),\,i\in S_p$ are in the range between
\begin{equation} \label{eq:signal-size}
    \underline{\Delta} = 2\underline{r}\log{p}
    \quad\text{and}\quad
    \overline{\Delta} = 2\overline{r}\log{p},
\end{equation}
for some constants $0<\underline{r}\le\overline{r}$.
Here $\beta$ parametrizes the problem sparsity, whereas $\underline{r}$ and $\overline{r}$ parametrize the signal sizes.

We define the criteria for success and failure of exact support recovery under this asymptotic regime.
\begin{definition} \label{def:exact-recovery-success-failure}
We say the exact support recovery asymptotically succeeds if 
\begin{equation} \label{eq:exact-recovery-success}
    \mathrm{risk}^{\mathrm{Exact}}\to0, \quad \text{as}\quad p\to\infty.
\end{equation}
We say the exact support recovery asymptotically fails if 
\begin{equation} \label{eq:exact-recovery-failure}
    \liminf\mathrm{risk}^{\mathrm{Exact}}\ge1, \quad \text{as}\quad p\to\infty.
\end{equation}
\end{definition}
Similarly, we define the criteria for asymptotic success and failure for approximate support recovery as follows.
\begin{definition} \label{def:approx-recovery-success-failure}
We say the approximate support recovery asymptotically succeeds if 
\begin{equation} \label{eq:approx-recovery-success}
    \mathrm{risk}^{\mathrm{Approx}}\to0, \quad \text{as}\quad p\to\infty.
\end{equation}
We say the approximate support recovery asymptotically fails if 
\begin{equation} \label{eq:approx-recovery-failure}
    \liminf\mathrm{risk}^{\mathrm{Approx}}\ge1, \quad \text{as}\quad p\to\infty.
\end{equation}
\end{definition}
The performance of procedures in terms of the criteria in Definitions \ref{def:exact-recovery-success-failure} and \ref{def:approx-recovery-success-failure} will be analyzed in Sections \ref{subsec:strong-classification-boundary} and \ref{subsec:weak-classification-boundary}, respectively.

\begin{remark}
The parametrization of signal sparsity \eqref{eq:signal-sparsity} and signal sizes  \eqref{eq:signal-size} was first introduced \citet{donoho2004higher}, where the signal sizes are assumed equal with a magnitude of $2{r}\log{p}$.
It was shown in \cite{donoho2004higher} that a phase transition in the $r$-$\beta$ plane exists for the signal detection problem. 
That is, depending on the $r$-$\beta$ combination, the global hypothesis test for $\lambda(i)=0$ for all $i$ can be either solved exactly, with vanishing type I and type II errors, or no better than a random guess, with the sum of type I and type II errors exceeding 1.

We shall see that the scaling of sparsity \eqref{eq:signal-sparsity} and signal size \eqref{eq:signal-size} is also the suitable parametrization for studying the phase transitions of the approximate and exact signal support recovery problems.
\end{remark}

\subsection{FWER-controlling procedures}
\label{subsec:FWER-controlling-procedures}

We review four thresholding procedures that aims at controlling family-wise error rates.
The first (deterministic) thresholding procedure is the well-known Bonferroni's procedure.
\begin{definition}[Bonferroni's procedure]
Suppose the errors $\epsilon(j)$'s have a common marginal distribution $F$, Bonferroni's procedure with level $\alpha$ is the thresholding procedure that uses the threshold
\begin{equation} \label{eq:Bonferroni-procedure}
    t_p = F^{\leftarrow}(1 - \alpha/p).
\end{equation}
%where  $F^{\leftarrow}(u)=\inf{\left\{x:F(x)\ge u\right\}}$ is the generalized inverse function.
\end{definition}
% It is easy to see that the family-wise error rate (FWER) is controlled at level $\alpha$ by applying the union bound, regardless of the error-dependence structure (see e.g.\ Relation \eqref{eq:Bonferroni-FWER-control}, below).
A closely related procedure is Sid\'ak's procedure \citep{vsidak1967rectangular}
which is a more aggressive (and also deterministic) thresholding procedure that uses the 
threshold
\begin{equation} \label{eq:Sidak-procedure}
    t_p = F^{\leftarrow}((1 - \alpha)^{1/p}).
\end{equation}
% can be shown to control FWER in the case independent errors.

A procedure strictly more powerful than Bonferroni's is the so-called Holm's procedure \citep{holm1979simple}.
On observing the data $x$, its coordinates can be ordered from largest to smallest
$x(i_1) \ge x(i_2)  \ge \ldots \ge x(i_p)$,
where $(i_1, \ldots, i_p)$ is a permutation of $\{1, \ldots, p\}$. 
\begin{definition}[Holm's procedure]
Let $k$ be the largest index such that
$$
\overline{F}(x(i_j)) \le \alpha / (p-j+1),\quad \text{for all}\;j\le k.
$$
Holm's procedure with level $\alpha$ is the thresholding procedure that uses the threshold
\begin{equation} \label{eq:Holm-procedure}
    t_p(x) = x(i_{k}),
\end{equation}
\end{definition}
In contrast to the Bonferroni procedure, Holm's procedure is data-dependent.
% It can be shown that Holm's procedure also controls FWER at $\alpha$ level, regardless of dependence in the data.

A closely related, more aggressive (data-dependent) thresholding procedure is Hochberg's procedure \citep{hochberg1988sharper}
%\begin{definition}[Hochberg's procedure]
%Hochberg's procedure 
which replaces the index $k$ in Holm's procedure with the largest index $j$ such that
$$
\overline{F}(x(i_j)) \le \alpha / (p-j+1).
$$
%where  $\overline{F}(x)=1-F(x)$ is the survival function.
%\end{definition}

\subsection{The strong classification boundary}
\label{subsec:strong-classification-boundary}

We first elaborate on the relationship between the probability of exact recovery and risk of exact support recovery, as promised in Section \ref{subsec:risks}.
\begin{lemma} \label{lemma:risk-exact-recovery-probability}
Let $\mathcal{R} = \mathcal{R}_p$ be the sequence of procedures for support recovery under the chi-square model \eqref{eq:model-chisq}. 
%The probability of exact recovery $\P[\widehat{S} = S]$, and risk of exact support recovery $\mathrm{risk}^{\mathrm{Exact}}$, defined in \eqref{eq:risk-exact}, are related as follows,
In this case, as $p\to\infty$, we have
\begin{equation} \label{eq:exact-recovery-implies-risk-0}
    \P[\widehat{S} = S] \to 1 \iff \mathrm{risk}^{\mathrm{Exact}}\to0,
\end{equation}
and
\begin{equation} \label{eq:failure-recovery-implies-risk-1}
    \P[\widehat{S} = S] \to 0 \implies \liminf\mathrm{risk}^{\mathrm{Exact}}\ge1,
\end{equation}
where the dependence on $p$ was suppressed for notational convenience.
\end{lemma}

\begin{proof}[Proof of Lemma \ref{lemma:risk-exact-recovery-probability}]
Notice that $\{\widehat{S}=S\}$ implies $\{\widehat{S}\subseteq S\} \cap \{\widehat{S}\supseteq S\}$, therefore we have for every fixed $p$,
\begin{equation} \label{eq:risk-exact-recovery-probability-proof-1}
    \mathrm{risk}^{\mathrm{Exact}} 
    = 2 - \P[\widehat{S} \subseteq S] - \P[S \subseteq \widehat{S}] \\
    \le 2 - 2\P[\{\widehat{S}=S\}].
\end{equation}
On the other hand, since $\{\widehat{S}\neq S\}$ implies $\{\widehat{S}\not\subseteq S\} \cup \{\widehat{S}\not\supseteq S\}$, we have for every fixed $p$,
\begin{equation} \label{eq:risk-exact-recovery-probability-proof-2}
    1 - \P[\{\widehat{S}=S\}]
    = \P[\{\widehat{S}\neq S\}]
    \le 2 - \P[\widehat{S} \subseteq S] - \P[S \subseteq \widehat{S}]
    = \mathrm{risk}^{\mathrm{Exact}}. 
\end{equation}
Relation \eqref{eq:exact-recovery-implies-risk-0} follows from \eqref{eq:risk-exact-recovery-probability-proof-1} and \eqref{eq:risk-exact-recovery-probability-proof-2}, and Relation \eqref{eq:failure-recovery-implies-risk-1} from \eqref{eq:risk-exact-recovery-probability-proof-2}.
\end{proof}

\begin{remark}
By Lemma \ref{lemma:risk-exact-recovery-probability}, is in fact sufficient to study the probability of exact support recovery $\P[\{\widehat{S}=S\}]$ in place of $\mathrm{risk}^{\mathrm{Exact}}$, if we are interested in the asymptotic properties of the risk defined in \eqref{def:exact-recovery-success-failure}.
The converse, however, is not true.
An example where $\liminf\mathrm{risk}^{\mathrm{Exact}}\ge1$ but $\P[\widehat{S} = S] \not\to 0$ is given in Remark \ref{rmk:asymptotic-risks}.
\end{remark}

We now state our first main result, characterizing the phase-transition  the exact support recovery problem.

\begin{theorem} \label{thm:chi-squred-strong-boundary}
Consider the high-dimensional chi-squared model \eqref{eq:model-chisq} with signal sparsity and size as described in \eqref{eq:signal-sparsity} and \eqref{eq:signal-size}.
The function 
\begin{equation} \label{eq:strong-classification-boundary-chisquared}
    g(\beta) = \left(\sqrt{1-\beta} + \sqrt{2-\beta}\right)^2
\end{equation}
characterizes the phase transition of exact support recovery problem.
Specifically, if $\underline{r} > {{g}}(\beta)$, then Bonferroni's procedure $\widehat{S}_p$ (defined in \eqref{eq:Bonferroni-procedure}) with FWER levels $\alpha=\alpha_p$ satisfying
\begin{equation} \label{eq:FWER-rate-to-zero}
    \alpha\to 0,\quad \text{and} \quad \alpha p^\delta\to\infty \text{  for any } \delta>0,
\end{equation}
achieves asymptotic perfect support recovery in the sense of \eqref{eq:exact-recovery-success}. 

Conversely, if $\overline{r} < {{g}}(\beta)$, for any procedure $\widehat{S}$, we have $\P[\widehat{S}=S]\to0$.
Therefore, in view of Lemma \ref{lemma:risk-exact-recovery-probability}, exact support recovery asymptotically fails for any and all procedures in the sense of \eqref{eq:exact-recovery-failure}.
\end{theorem}

The strong classification boundary is shown in Figure \ref{fig:phase-chi-squared}

\begin{figure}
      \centering
      \includegraphics[width=0.6\textwidth]{./phase_diagram_chisquared.eps}
      % \includegraphics[width=0.35\textwidth]{./phase_diagram_chisquared.eps}
      \caption{The boundary described in Theorems \ref{thm:chi-squred-strong-boundary} and \ref{thm:chi-squred-weak-boundary}. 
      Remarkbly, the boundary is unaffected by the degrees-of-freedom parameter.
      For the detection boundary, see \citep{donoho2004higher}.} 
      \label{fig:phase-chi-squared}
\end{figure}

It is interesting to note that signal sizes in the chi-squared model grows at the same rate as the Laplace model.
The asymmetry in the chi-squared model can be seen in \eqref{eq:strong-classification-boundary-chisquared}: as $\beta\to0$, the boundary approaches $3+2\sqrt{2} \approx 5.828$ instead of $2$ as in the case model of additive errors with Laplace errors; the required signal size is much larger.

A remarkable feature of the boundary \eqref{eq:strong-classification-boundary-chisquared} is that it is independent of the degrees of freedom $\nu$.
In simulations, we find the approximation by the asymptotic boundary to be quite accurate even in moderate dimensions, and largely unaffected by the degrees of freedom.
See Figure \ref{fig:phase-simulated-chi-squared} for numerical illustrations.

\subsection{FDR-controlling procedures}
\label{subsec:FDR-controlling-procedures}

Introduce the Benjamini-Hochberg procedure here.

\subsection{The weak classification boundary}
\label{subsec:weak-classification-boundary}

\begin{theorem} \label{thm:chi-squred-weak-boundary}
Consider the high-dimensional chi-squared model \eqref{eq:model-chisq}.
Let the signal $\lambda$ be as described in Theorem \ref{thm:chi-squred-strong-boundary}.
The function 
\begin{equation} \label{eq:weak-classification-boundary-chisquared}
    h(\beta) = \beta
\end{equation}
characterizes the phase transition of approximate support recovery problem.
Specifically, if $\underline{r} > {h}(\beta)$, then the Benjamini-Hochberg procedure $\widehat{S}_p$ (defined in \eqref{eq:BH-procedure}) with FDR levels $\alpha=\alpha_p$ satisfying
\begin{equation} \label{eq:FDR-rate-to-zero}
    \alpha\to 0,\quad \text{and} \quad \alpha p^\delta\to\infty \text{  for any } \delta>0.
\end{equation}
achieves asymptotically approximate support recovery in the sense of \eqref{eq:approx-recovery-success}. 

Conversely, if $\overline{r} < {h}(\beta)$, approximate support recovery asymptotically fails in the sense of \eqref{eq:approx-recovery-failure} for any and all procedures.
\end{theorem}
